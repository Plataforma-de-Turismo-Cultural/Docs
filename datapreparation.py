# -*- coding: utf-8 -*-
"""DataPreparation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d09gX3bw_7ElAzBv2XEs-gaOgUJPiVUh
"""

# === 1.Importando bibliotecas ===
import pandas as pd
import numpy as np

# === Carregamento dos dados ===
users = pd.read_csv("users.csv")
posts = pd.read_csv("posts.csv")
interactions = pd.read_csv("interactions.csv")
destinations = pd.read_csv("destinations.csv")
agencies = pd.read_csv("agencies.csv")
gender = pd.read_csv("gender.csv")

# === 2.Inspeção inicial ===
users.info()

posts.head()

posts.isnull().sum()

# === 3.Tratamento de valores ausentes ===
users['fk_gender_id'].fillna(users['fk_gender_id'].mode()[0], inplace=True)
posts['post_type'].fillna("not_specified", inplace=True)
posts['rating'].fillna(posts['rating'].median(), inplace=True)
agencies['name'].fillna("Unknown Agency", inplace=True)

# === 4.Remoção de duplicados ===
users.drop_duplicates(subset=['username', 'email'], inplace=True)
posts.drop_duplicates(subset=['fk_user_id', 'content'], inplace=True)
interactions.drop_duplicates(subset=['fk_user_id', 'fk_post_id'], inplace=True)

# === 5.Padronização de formatos ===
users['username'] = users['username'].str.lower().str.strip()
posts['post_type'] = posts['post_type'].str.lower().str.replace(" ", "_")
posts['created_at'] = pd.to_datetime(posts['created_at'], errors='coerce')
gender['description'] = gender['description'].str.title()

# === 6.Tratamento de outliers ===
Q1 = posts['rating'].quantile(0.25)
Q3 = posts['rating'].quantile(0.75)
IQR = Q3 - Q1
limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

posts = posts[(posts['rating'] >= limite_inferior) & (posts['rating'] <= limite_superior)]

# === 7.Verificação de consistência relacional ===
valid_users = set(users['user_id'])
posts = posts[posts['fk_user_id'].isin(valid_users)]

valid_posts = set(posts['post_id'])
interactions = interactions[interactions['fk_post_id'].isin(valid_posts)]

interactions

# === 8.Análise Exploratória de Dados (EDA) ===
import matplotlib.pyplot as plt
import seaborn as sns

# 9.Configurações visuais
sns.set(style="whitegrid")
plt.rcParams['figure.figsize'] = (8, 5)

# === Estatísticas descritivas básicas ===
#--- Estatísticas descritivas das avaliações ---
posts['rating'].describe()

#--- Contagem de tipos de post ---
posts['post_type'].value_counts()

# === 8.2 Distribuição dos tipos de post ===
plt.figure(figsize=(8,4))
sns.countplot(data=posts, x='post_type', order=posts['post_type'].value_counts().index, palette='Set2')
plt.title("Distribuição dos Tipos de Publicação")
plt.xlabel("Tipo de Post")
plt.ylabel("Frequência")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# === 8.3 Distribuição das avaliações ===
plt.figure(figsize=(7,4))
sns.histplot(posts['rating'], bins=10, kde=True, color='skyblue')
plt.title("Distribuição das Avaliações dos Posts")
plt.xlabel("Avaliação")
plt.ylabel("Contagem")
plt.show()

# === 8.4 Boxplot para detecção visual de outliers ===
plt.figure(figsize=(6,4))
sns.boxplot(x=posts['rating'], color='lightgreen')
plt.title("Boxplot das Avaliações dos Posts")
plt.xlabel("Avaliação")
plt.show()

# === 8.5 Relação entre tipo de post e média de avaliação ===
avg_rating = posts.groupby('post_type')['rating'].mean().sort_values(ascending=False)
sns.barplot(x=avg_rating.index, y=avg_rating.values, palette='coolwarm')
plt.title("Média de Avaliação por Tipo de Post")
plt.xlabel("Tipo de Post")
plt.ylabel("Média de Avaliação")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# === 8.6 Top 10 utilizadores mais ativos ===
top_users = posts['fk_user_id'].value_counts().head(10)
sns.barplot(x=top_users.index, y=top_users.values, palette='viridis')
plt.title("Top 10 Utilizadores mais Ativos")
plt.xlabel("ID do Utilizador")
plt.ylabel("Número de Publicações")
plt.show()

# === 8.7 Correlação entre variáveis numéricas ===
numeric_cols = posts.select_dtypes(include=np.number)
corr = numeric_cols.corr()

plt.figure(figsize=(6,4))
sns.heatmap(corr, annot=True, cmap='Blues', fmt=".2f")
plt.title("Mapa de Correlação entre Variáveis Numéricas")
plt.show()

#Codificação de variáveis categóricas
posts = pd.get_dummies(posts, columns=['post_type'], drop_first=True)
users = pd.get_dummies(users, columns=['fk_gender_id'], drop_first=True)

#Extração de características temporais
posts['created_at'] = pd.to_datetime(posts['created_at'])
posts['post_hour'] = posts['created_at'].dt.hour
posts['post_day'] = posts['created_at'].dt.day_name()
posts['post_month'] = posts['created_at'].dt.month

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numeric_features = ['rating', 'post_length', 'post_hour', 'post_month']
posts[numeric_features] = scaler.fit_transform(posts[numeric_features])

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Selecionar colunas numéricas + criar dummies para categóricas
X = posts.drop(columns=['post_id', 'fk_user_id', 'content', 'created_at', 'rating'])
X = pd.get_dummies(X, drop_first=True)  # converte strings em 0/1

y = posts['rating']

# Divisão treino/teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest
rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)

# Predição e métricas
y_pred = rf_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.3f}")
print(f"R² Score: {r2:.3f}")

# Importância das features
importances = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
print(importances.head(10))